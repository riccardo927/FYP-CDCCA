{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "gpuClass": "premium",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "0uOtqrEgATxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sjLjDY3Ag2dn"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import scipy\n",
        "from scipy.linalg import sqrtm, inv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "\n",
        "torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(yf.__version__)\n",
        "print(np.__version__)\n",
        "print(pd.__version__)\n",
        "print(scipy.__version__)\n",
        "\n",
        "print(torch.__version__)\n",
        "print(sns.__version__)\n",
        "print(matplotlib.__version__)\n",
        "print(sklearn.__version__)"
      ],
      "metadata": {
        "id": "A3NUZttHsFav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing functions"
      ],
      "metadata": {
        "id": "dOzWLmEWBCYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_data(ticker, start_date, end_date):\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    return data['Close'].dropna()\n",
        "\n",
        "def fetch_data_volume(ticker, start_date, end_date):\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    return data['Volume'].dropna()\n",
        "\n",
        "def calculate_changes(price_data):\n",
        "    return price_data.diff()#.dropna()\n",
        "\n",
        "def calculate_daily_returns(price_data):\n",
        "    return price_data.pct_change()#.dropna()\n",
        "\n",
        "def calculate_rate_of_change(daily_returns):\n",
        "    return daily_returns.diff()#.dropna()\n",
        "\n",
        "def create_complex_numbers(real_part, imaginary_part):\n",
        "    return real_part + 1j * imaginary_part\n",
        "\n",
        "def load_data(name, start_date = '1992-01-01', end_date = '2023-01-01'):\n",
        "    print(\"loading data ...\")\n",
        "    data = fetch_data(name, start_date, end_date)\n",
        "    daily_returns = calculate_daily_returns(data)\n",
        "    roc = calculate_rate_of_change(daily_returns)\n",
        "\n",
        "    diff = data.pct_change()\n",
        "\n",
        "    data_norm = (data-data.min())/(data.max()-data.min())\n",
        "    diff_norm = (diff-diff.min())/(diff.max()-diff.min())\n",
        "\n",
        "    aligned_data = pd.concat([data_norm[1:], diff_norm], axis=1).dropna()\n",
        "    aligned_data.columns = ['Close', 'Change']\n",
        "\n",
        "    complex_data = (create_complex_numbers(aligned_data['Close'], aligned_data['Change'])).to_numpy()\n",
        "\n",
        "    return complex_data\n",
        "\n",
        "def split_data(data, train_ratio=0.7, val_ratio=0.15):\n",
        "    data_length = len(data)\n",
        "    train_end = int(data_length * train_ratio)\n",
        "    val_end = int(data_length * (train_ratio + val_ratio))\n",
        "\n",
        "    train_data = data[:train_end]\n",
        "    val_data = data[train_end:val_end]\n",
        "    test_data = data[val_end:]\n",
        "\n",
        "    return torch.tensor(train_data).unsqueeze(1), torch.tensor(val_data).unsqueeze(1), torch.tensor(test_data).unsqueeze(1)\n"
      ],
      "metadata": {
        "id": "s3B03iaNt3nI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Old load_data() functions\n",
        "\n",
        "# used to develop best performing one\n",
        "def load_data1(name, start_date = '1992-01-01', end_date = '2023-01-01'):\n",
        "    print(\"loading data ...\")\n",
        "    # Fetch financial data\n",
        "    data = fetch_data(name, start_date, end_date)\n",
        "\n",
        "    # Calculate daily returns\n",
        "    daily_returns = calculate_daily_returns(data)\n",
        "    # Calculate rate of change of daily returns\n",
        "    roc = calculate_rate_of_change(daily_returns)\n",
        "\n",
        "    # Align data and drop missing values\n",
        "    aligned_data = pd.concat([daily_returns[2:], roc[2:]], axis=1).dropna()\n",
        "    aligned_data.columns = ['Day Ret', 'ROC']\n",
        "\n",
        "    standard_scaler = StandardScaler()\n",
        "    scaled = standard_scaler.fit_transform(aligned_data.values)\n",
        "\n",
        "    scaled_features_df = pd.DataFrame(scaled, index=aligned_data.index, columns=aligned_data.columns)\n",
        "\n",
        "    # Create complex numbers\n",
        "    complex_data = (create_complex_numbers(scaled_features_df['Day Ret'], scaled_features_df['ROC'])).to_numpy()\n",
        "\n",
        "    return complex_data\n",
        "\n",
        "def load_data2(name, start_date = '1992-01-01', end_date = '2023-01-01'):\n",
        "    print(\"loading data ...\")\n",
        "    data = fetch_data(name, start_date, end_date)\n",
        "    daily_returns = calculate_daily_returns(data)\n",
        "    roc = calculate_rate_of_change(daily_returns)\n",
        "\n",
        "    diff = data.diff()\n",
        "\n",
        "    data_norm = (data-data.mean())/ data.std() #(data-data.min())/(data.max()-data.min())\n",
        "    diff_norm = (diff-diff.mean())/ diff.std() #(diff-diff.min())/(diff.max()-diff.min())\n",
        "\n",
        "    aligned_data = pd.concat([data_norm[1:], diff_norm], axis=1).dropna()\n",
        "    aligned_data.columns = ['Close', 'Change']\n",
        "\n",
        "    complex_data = (create_complex_numbers(aligned_data['Close'], aligned_data['Change'])).to_numpy()\n",
        "\n",
        "    return complex_data\n",
        "\n",
        "def load_data3(name, start_date = '1992-01-01', end_date = '2023-01-01'):\n",
        "    print(\"loading data ...\")\n",
        "    # Fetch financial data\n",
        "    data = fetch_data(name, start_date, end_date)\n",
        "    volume = fetch_data_volume(name, start_date, end_date)\n",
        "    zeros = pd.Series(0, index = data.index)\n",
        "\n",
        "    log_returns = np.log(data).diff()\n",
        "    log_volumes = np.log(volume).diff()\n",
        "\n",
        "    daily_returns = calculate_daily_returns(data)\n",
        "    roc = calculate_rate_of_change(daily_returns)\n",
        "    aligned_data = pd.concat([log_returns, zeros], axis=1).dropna()\n",
        "    aligned_data.columns = ['Data1', 'Data2']\n",
        "\n",
        "    standard_scaler = StandardScaler()\n",
        "    scaled = standard_scaler.fit_transform(aligned_data.values)\n",
        "\n",
        "    scaled_features_df = pd.DataFrame(scaled, index=aligned_data.index, columns=aligned_data.columns)\n",
        "\n",
        "    # Create complex numbers\n",
        "    complex_data = (create_complex_numbers(scaled_features_df['Data1'], scaled_features_df['Data2'])).to_numpy()\n",
        "\n",
        "    #complex_data = (create_complex_numbers(aligned_data['Data1'], aligned_data['Data2'])).to_numpy()\n",
        "    return complex_data\n",
        "\n",
        "def load_data4(name, start_date = '1992-01-01', end_date = '2023-01-01'):\n",
        "    print(\"loading data ...\")\n",
        "    data = fetch_data(name, start_date, end_date)\n",
        "    dataset = (data-data.mean())/data.std()\n",
        "\n",
        "    log_returns = np.log(data).diff()\n",
        "\n",
        "    aligned_data = pd.concat([dataset[1:], log_returns[1:]], axis=1).dropna()\n",
        "    aligned_data.columns = ['Data1', 'Data2']\n",
        "\n",
        "    complex_data = (create_complex_numbers(aligned_data['Data1'], aligned_data['Data2'])).to_numpy()\n",
        "\n",
        "    return complex_data"
      ],
      "metadata": {
        "id": "caC2ovSt_kln"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Widely Linear Layer and Activation Functions"
      ],
      "metadata": {
        "id": "8Bm8WZbRBaO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define backpropagation using CR-calculus\n",
        "class WidelyLinearFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight1, weight2, bias):\n",
        "        ctx.save_for_backward(input, weight1, weight2, bias)\n",
        "        z = torch.mm(input, weight1.t()) + torch.mm(input.conj(), weight2.t()) + bias\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, weight1, weight2, bias = ctx.saved_tensors\n",
        "        grad_input = grad_weight1 = grad_weight2 = grad_bias = None\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = (grad_output.conj().mm(weight1) + grad_output.mm(weight2.conj())).conj()\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight1 = grad_output.t().mm(input.conj())\n",
        "        if ctx.needs_input_grad[2]:\n",
        "            grad_weight2 = grad_output.t().mm(input)\n",
        "        if bias is not None and ctx.needs_input_grad[3]:\n",
        "            grad_bias = grad_output.sum(0)\n",
        "\n",
        "        return grad_input, grad_weight1, grad_weight2, grad_bias\n",
        "\n",
        "\n",
        "class WidelyLinearLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(WidelyLinearLayer, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight1 = nn.Parameter(torch.complex(torch.Tensor(out_features, in_features),\n",
        "                                                  torch.Tensor(out_features, in_features)))\n",
        "        self.weight2 = nn.Parameter(torch.complex(torch.Tensor(out_features, in_features),\n",
        "                                                  torch.Tensor(out_features, in_features)))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.complex(torch.Tensor(out_features),\n",
        "                                                   torch.Tensor(out_features)))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight1.real, a=math.sqrt(5))#, nonlinearity='leaky_relu')\n",
        "        nn.init.kaiming_uniform_(self.weight1.imag, a=math.sqrt(5))#, nonlinearity='leaky_relu')\n",
        "        nn.init.kaiming_uniform_(self.weight2.real, a=math.sqrt(5))#, nonlinearity='leaky_relu')\n",
        "        nn.init.kaiming_uniform_(self.weight2.imag, a=math.sqrt(5))#, nonlinearity='leaky_relu')\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight1.real)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias.real, -bound, bound)\n",
        "            nn.init.uniform_(self.bias.imag, -bound, bound)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # z = torch.mm(input, self.weight1.t()) + torch.mm(input.conj(), self.weight2.t()) + self.bias\n",
        "        # return z\n",
        "        return WidelyLinearFunction.apply(input, self.weight1, self.weight2, self.bias)\n",
        "\n",
        "class ComplexArctan(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, z):\n",
        "        ctx.save_for_backward(z)\n",
        "        output = torch.atan(z)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        z, = ctx.saved_tensors\n",
        "        grad_input = (grad_output.conj() / (1 + torch.square(z)))\n",
        "        return grad_input.conj()\n",
        "\n",
        "class ComplexSigmoid(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, z):\n",
        "        ctx.save_for_backward(z)\n",
        "        output = torch.sigmoid(z)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        z, = ctx.saved_tensors\n",
        "        grad_input = grad_output.conj() * torch.sigmoid(z) * (1 - torch.sigmoid(z))\n",
        "        return grad_input.conj()\n",
        "\n",
        "class ComplexActivation(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return torch.atan(input)\n",
        "        #return ComplexArctan.apply(input)\n",
        "        #return ComplexSigmoid.apply(input)\n",
        "\n",
        "class ComplexRelu(nn.Module):\n",
        "    def forward(self, input):\n",
        "        f = nn.ReLU()\n",
        "        z = torch.view_as_real(input)\n",
        "        f_z = f(z)\n",
        "        return torch.view_as_complex(f_z)\n",
        "\n",
        "# class ComplexBatchNorm1d(nn.Module):\n",
        "#     def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n",
        "#         super(ComplexBatchNorm1d, self).__init__()\n",
        "#         self.bn_r = nn.BatchNorm1d(num_features, eps=eps, momentum=momentum, affine=affine)\n",
        "#         self.bn_i = nn.BatchNorm1d(num_features, eps=eps, momentum=momentum, affine=affine)\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         output = self.bn_r(input.real) + 1j*self.bn_i(input.imag)\n",
        "#         return output"
      ],
      "metadata": {
        "id": "DGnUhMylBZfO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient checks"
      ],
      "metadata": {
        "id": "BD0iIFZHD0TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import gradcheck\n",
        "\n",
        "widely = WidelyLinearFunction.apply\n",
        "\n",
        "input = (torch.randn(100,1,dtype=torch.cdouble,requires_grad=True), torch.randn(128,1,dtype=torch.cdouble,requires_grad=True), torch.randn(128,1,dtype=torch.cdouble,requires_grad=True), torch.randn(128,dtype=torch.cdouble,requires_grad=True))\n",
        "test = gradcheck(widely, input, eps=1e-6, atol=1e-4)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgJlszf-Dzba",
        "outputId": "a640a909-3bee-4806-de27-8a7fef593a07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import gradcheck\n",
        "\n",
        "# change activation to test\n",
        "activation = ComplexSigmoid.apply #ComplexArctan.apply\n",
        "\n",
        "input = torch.randn(100,1,dtype=torch.cdouble,requires_grad=True)\n",
        "test = gradcheck(activation, input, eps=1e-6, atol=1e-4)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b83QtrjMD5ya",
        "outputId": "89b23d5b-c9ab-4548-ceba-6f4c579f06bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CDCCA Loss"
      ],
      "metadata": {
        "id": "d7hOby6dBtJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cdcca_loss():\n",
        "    def __init__(self, outdim_size, device):\n",
        "        self.outdim_size = outdim_size\n",
        "        self.device = device\n",
        "\n",
        "    def loss(self, H1, H2):\n",
        "\n",
        "        r1 = 1e-3 + 1j*1e-5\n",
        "        r2 = 1e-3 + 1j*1e-5\n",
        "        eps = 1e-3\n",
        "\n",
        "        H1, H2 = H1.t(), H2.t()\n",
        "\n",
        "        o1 = o2 = H1.size(0)\n",
        "        m = H1.size(1)\n",
        "\n",
        "        H1bar = H1 - H1.mean(dim=1).unsqueeze(dim=1)\n",
        "        H2bar = H2 - H2.mean(dim=1).unsqueeze(dim=1)\n",
        "\n",
        "        # Compute covariance matrices and add regularization term so they are positive definite\n",
        "        SigmaHat12 = (1.0 / (m - 1)) * torch.matmul(H1bar, H2bar.conj().t())\n",
        "        SigmaHat11 = (1.0 / (m - 1)) * torch.matmul(H1bar, H1bar.conj().t()) + r1 * torch.eye(o1, device=self.device)\n",
        "        SigmaHat22 = (1.0 / (m - 1)) * torch.matmul(H2bar, H2bar.conj().t()) + r2 * torch.eye(o2, device=self.device)\n",
        "\n",
        "        SigmaTilde12 = (1.0 / (m - 1)) * torch.matmul(H1bar, H2bar.t())\n",
        "        SigmaTilde11 = (1.0 / (m - 1)) * torch.matmul(H1bar, H1bar.t()) + r1 * torch.eye(o1, device=self.device)\n",
        "        SigmaTilde22 = (1.0 / (m - 1)) * torch.matmul(H2bar, H2bar.t()) + r2 * torch.eye(o2, device=self.device)\n",
        "\n",
        "        AugmentedSigma12 = torch.cat((torch.cat((SigmaHat12, SigmaTilde12), 1),torch.cat((torch.conj(SigmaTilde12), torch.conj(SigmaHat12)), 1)), 0)\n",
        "        AugmentedSigma11 = torch.cat((torch.cat((SigmaHat11, SigmaTilde11), 1),torch.cat((torch.conj(SigmaTilde11), torch.conj(SigmaHat11)), 1)), 0)\n",
        "        AugmentedSigma22 = torch.cat((torch.cat((SigmaHat22, SigmaTilde22), 1),torch.cat((torch.conj(SigmaTilde22), torch.conj(SigmaHat22)), 1)), 0)\n",
        "\n",
        "        [D1, V1] = torch.linalg.eig(AugmentedSigma11)\n",
        "        [D2, V2] = torch.linalg.eig(AugmentedSigma22)\n",
        "\n",
        "        AugSigma11RootInv = torch.matmul(torch.matmul(V1, torch.diag(D1 ** -0.5)), V1.conj().t())\n",
        "        AugSigma22RootInv = torch.matmul(torch.matmul(V2, torch.diag(D2 ** -0.5)), V2.conj().t())\n",
        "\n",
        "        Tval = torch.matmul(torch.matmul(AugSigma11RootInv, AugmentedSigma12), AugSigma22RootInv)\n",
        "\n",
        "        C = (1/np.sqrt(2)) * torch.cat((torch.cat((torch.eye(o1, device=self.device), torch.complex(torch.zeros(o1, o1, device = self.device), torch.eye(o1, device=self.device))), 1),\n",
        "                                        torch.cat((torch.eye(o1, device=self.device), torch.complex(torch.zeros(o1, o1, device = self.device), -torch.eye(o1, device=self.device))), 1)), 0)\n",
        "\n",
        "        T_hat = torch.matmul(torch.matmul(C.conj().t(), Tval), C)\n",
        "        T_hat = T_hat.real\n",
        "\n",
        "        [_, K, _] = torch.linalg.svd(T_hat)\n",
        "\n",
        "        corr = 1/(o1*2) * torch.sum(K)\n",
        "\n",
        "        return - corr.real"
      ],
      "metadata": {
        "id": "clVx85xYBw7L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Network and ComplexDeepCCA"
      ],
      "metadata": {
        "id": "wgJ_9Vw3Bhu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MlpNetwork(nn.Module):\n",
        "    def __init__(self, layer_sizes, input_size):\n",
        "        super(MlpNetwork, self).__init__()\n",
        "        layers = []\n",
        "        layer_sizes = [input_size] + layer_sizes\n",
        "        self.complex_activation = ComplexRelu()\n",
        "        for l_id in range(len(layer_sizes) - 1):\n",
        "            if l_id == len(layer_sizes) - 2:\n",
        "                layers.append(nn.Sequential(\n",
        "                    #ComplexBatchNorm1d(num_features=layer_sizes[l_id], affine=False),\n",
        "                    WidelyLinearLayer(layer_sizes[l_id], layer_sizes[l_id + 1]),\n",
        "                ))\n",
        "            else:\n",
        "                layers.append(nn.Sequential(\n",
        "                    WidelyLinearLayer(layer_sizes[l_id], layer_sizes[l_id + 1]),\n",
        "                    self.complex_activation,\n",
        "                    #ComplexBatchNorm1d(num_features=layer_sizes[l_id + 1], affine=False),\n",
        "                ))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ComplexDeepCCA(nn.Module):\n",
        "    def __init__(self, layer_sizes1, layer_sizes2, input_size1, input_size2, outdim_size, device=torch.device('cpu')):\n",
        "        super(ComplexDeepCCA, self).__init__()\n",
        "        self.model1 = MlpNetwork(layer_sizes1, input_size1).double()\n",
        "        self.model2 = MlpNetwork(layer_sizes2, input_size2).double()\n",
        "\n",
        "        self.loss = cdcca_loss(outdim_size, device).loss\n",
        "\n",
        "    def forward(self, x1, x2, device):\n",
        "        input1 = torch.randn(x1.size(0),1, dtype=torch.cdouble)\n",
        "        input2 = torch.randn(x2.size(0),1, dtype=torch.cdouble)\n",
        "\n",
        "        for i in range(0,len(input1)):\n",
        "            input1[i] = torch.complex(x1[i][0][0], x1[i][0][1])\n",
        "            input2[i] = torch.complex(x2[i][0][0], x2[i][0][1])\n",
        "\n",
        "        output1 = self.model1(input1.to(device=device))\n",
        "        output2 = self.model2(input2.to(device=device))\n",
        "\n",
        "        return output1, output2"
      ],
      "metadata": {
        "id": "u8RETHisBZbf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear CCA"
      ],
      "metadata": {
        "id": "zx0_qmIXBVKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class linear_cca():\n",
        "    def __init__(self):\n",
        "        self.w = [None, None]\n",
        "        self.m = [None, None]\n",
        "\n",
        "    def fit(self, H1, H2, outdim_size):\n",
        "\n",
        "        r1 = 1e-4 + 1j*1e-6\n",
        "        r2 = 1e-4 + 1j*1e-6\n",
        "\n",
        "        m = H1.shape[0]\n",
        "        o1 = H1.shape[1]\n",
        "        o2 = H2.shape[1]\n",
        "\n",
        "        self.m[0] = np.mean(H1, axis=0)\n",
        "        self.m[1] = np.mean(H2, axis=0)\n",
        "        H1bar = H1 - np.tile(self.m[0], (m, 1))\n",
        "        H2bar = H2 - np.tile(self.m[1], (m, 1))\n",
        "\n",
        "        SigmaHat12 = (1.0 / (m - 1)) * np.dot(H1bar.conjugate().T, H2bar)\n",
        "        SigmaHat11 = (1.0 / (m - 1)) * np.dot(H1bar.conjugate().T, H1bar) + r1 * np.identity(o1)\n",
        "        SigmaHat22 = (1.0 / (m - 1)) * np.dot(H2bar.conjugate().T, H2bar) + r2 * np.identity(o2)\n",
        "\n",
        "        SigmaTilde12 = (1.0 / (m - 1)) * np.dot(H1bar.T, H2bar)\n",
        "        SigmaTilde11 = (1.0 / (m - 1)) * np.dot(H1bar.T, H1bar) + r1 * np.identity(o1)\n",
        "        SigmaTilde22 = (1.0 / (m - 1)) * np.dot(H2bar.T, H2bar) + r2 * np.identity(o2)\n",
        "\n",
        "        AugmentedSigma12 = np.concatenate((np.concatenate((SigmaHat12, SigmaTilde12), axis=1),\n",
        "                                           np.concatenate((SigmaTilde12.conjugate(), SigmaHat12.conjugate()), axis=1)), axis = 0)\n",
        "        AugmentedSigma11 = np.concatenate((np.concatenate((SigmaHat11, SigmaTilde11), axis=1),\n",
        "                                           np.concatenate((SigmaTilde11.conjugate(), SigmaHat11.conjugate()), axis=1)), axis = 0)\n",
        "        AugmentedSigma22 = np.concatenate((np.concatenate((SigmaHat22, SigmaTilde22), axis=1),\n",
        "                                           np.concatenate((SigmaTilde22.conjugate(), SigmaHat22.conjugate()), axis=1)), axis = 0)\n",
        "\n",
        "        [D1, V1] = scipy.linalg.eig(AugmentedSigma11)\n",
        "        [D2, V2] = scipy.linalg.eig(AugmentedSigma22)\n",
        "\n",
        "        AugSigma11RootInv = np.dot(np.dot(V1, np.diag(D1 ** -0.5)), V1.conjugate().T)\n",
        "        AugSigma22RootInv = np.dot(np.dot(V2, np.diag(D2 ** -0.5)), V2.conjugate().T)\n",
        "\n",
        "        Tval = np.dot(np.dot(AugSigma11RootInv, AugmentedSigma12), AugSigma22RootInv)\n",
        "\n",
        "        C = (1/np.sqrt(2)) * np.concatenate((np.concatenate((np.identity(o1), 1j*np.identity(o1)), axis=1),\n",
        "                                             np.concatenate((np.identity(o1), -1j*np.identity(o1)), axis=1)), axis = 0)\n",
        "\n",
        "        T_hat = np.dot(np.dot(C.conjugate().T, Tval), C)\n",
        "        T_hat = T_hat.real\n",
        "\n",
        "        [U_hat, K, V_hat] = scipy.linalg.svd(T_hat)\n",
        "        V_hat = V_hat.conjugate().T\n",
        "\n",
        "        U = np.dot(np.dot(C, U_hat), C.conjugate().T)\n",
        "        V = np.dot(np.dot(C, V_hat), C.conjugate().T)\n",
        "\n",
        "        A = np.dot(U.conjugate().T, AugSigma11RootInv)\n",
        "        B = np.dot(V.conjugate().T, AugSigma22RootInv)\n",
        "\n",
        "        self.w[0] = [A[0:outdim_size, 0:outdim_size], A[0:outdim_size, outdim_size:A.shape[1]]]\n",
        "        self.w[1] = [B[0:outdim_size, 0:outdim_size], B[0:outdim_size, outdim_size:A.shape[1]]]\n",
        "\n",
        "    def _get_result(self, x, idx):\n",
        "        input = x - self.m[idx].reshape([1, -1]).repeat(len(x), axis=0)\n",
        "        input_conj = input.conjugate()\n",
        "        result = np.dot(input, self.w[idx][0]) + np.dot(input_conj, self.w[idx][1])\n",
        "        return result\n",
        "\n",
        "    def transform(self, H1, H2):\n",
        "        return self._get_result(H1, 0), self._get_result(H2, 1)"
      ],
      "metadata": {
        "id": "zkwnuae6hicC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "2L4Ajm-YB8BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, linear_cca, outdim_size, epoch_num, batch_size, learning_rate, reg_par, device=torch.device('cpu')):\n",
        "        self.model = nn.DataParallel(model) #nn.parallel.DistributedDataParallel(model)\n",
        "        self.model.to(device)\n",
        "        self.epoch_num = epoch_num\n",
        "        self.batch_size = batch_size\n",
        "        self.loss = model.loss\n",
        "        self.optimizer = torch.optim.RMSprop(self.model.parameters(), lr=learning_rate, weight_decay=reg_par)\n",
        "        # self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.device = device\n",
        "\n",
        "        self.linear_cca = linear_cca\n",
        "        self.outdim_size = outdim_size\n",
        "\n",
        "        #create a logger\n",
        "        self.logger = logging.getLogger('mylogger')\n",
        "        level = logging.INFO #logging.DEBUG\n",
        "        self.logger.setLevel(level)\n",
        "\n",
        "        if (self.logger.hasHandlers()):\n",
        "            self.logger.handlers.clear()\n",
        "\n",
        "        handler = logging.FileHandler('DCCA.log')\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        handler.setFormatter(formatter)\n",
        "        self.logger.addHandler(handler)\n",
        "        self.logger.propagate = False\n",
        "\n",
        "        ch = logging.StreamHandler()\n",
        "        ch.setLevel(level)\n",
        "        self.logger.addHandler(ch)\n",
        "\n",
        "        self.logger.info(self.model)\n",
        "        self.logger.info(\"Number of model parameters: {}\".format(len(list(self.model.parameters()))))\n",
        "        self.logger.info(\"Model Parameters:\")\n",
        "        for i in range(len(list(self.model.parameters()))):\n",
        "            self.logger.info(list(self.model.parameters())[i].size())\n",
        "\n",
        "        self.logger.info(self.optimizer)\n",
        "\n",
        "    def train(self, x1, x2, vx1=None, vx2=None, tx1=None, tx2=None, checkpoint='checkpoint.model'):\n",
        "        \"\"\"\n",
        "        x1, x2 are the vectors needs to be make correlated\n",
        "        dim=[batch_size, feats]\n",
        "        \"\"\"\n",
        "        x1.to(self.device, dtype = torch.cfloat)\n",
        "        x2.to(self.device, dtype = torch.cfloat)\n",
        "\n",
        "        data_size = x1.size(0)\n",
        "\n",
        "        if vx1 is not None and vx2 is not None:\n",
        "            best_val_loss = 0\n",
        "            vx1.to(self.device)\n",
        "            vx2.to(self.device)\n",
        "        if tx1 is not None and tx2 is not None:\n",
        "            tx1.to(self.device)\n",
        "            tx2.to(self.device)\n",
        "\n",
        "        train_losses = []\n",
        "        train_loss_history = []\n",
        "        val_loss_history = []\n",
        "        for epoch in range(self.epoch_num):\n",
        "            epoch_start_time = time.time()\n",
        "            self.model.train()\n",
        "            batch_idxs = list(BatchSampler(SequentialSampler(range(data_size)), batch_size=self.batch_size, drop_last=False)) #RandomSampler #SequentialSampler\n",
        "            #with torch.autograd.profiler.profile() as prof:\n",
        "            for batch_idx in batch_idxs:\n",
        "                self.optimizer.zero_grad()\n",
        "                batch_x1 = x1[batch_idx, :]\n",
        "                batch_x2 = x2[batch_idx, :]\n",
        "                self.optimizer.zero_grad()\n",
        "                out1, out2 = self.model(batch_x1, batch_x2, self.device)\n",
        "                loss = self.loss(out1, out2)\n",
        "                train_losses.append(loss.item())\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            train_loss = np.mean(train_losses)\n",
        "            #print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
        "\n",
        "            info_string = \"Epoch {:d}/{:d} - time: {:.2f} - training_loss: {:.4f}\"\n",
        "            if vx1 is not None and vx2 is not None:\n",
        "                with torch.no_grad():\n",
        "                    self.model.eval()\n",
        "                    val_loss = self.transform(vx1, vx2)\n",
        "                    info_string += \" - val_loss: {:.4f}\".format(-val_loss)\n",
        "                    if val_loss < best_val_loss:\n",
        "                        self.logger.info(\n",
        "                            \"Epoch {:d}: val_loss improved from {:.4f} to {:.4f}, saving model to {}\".format(epoch + 1, -best_val_loss, -val_loss, checkpoint))\n",
        "                        best_val_loss = val_loss\n",
        "                        torch.save(self.model.state_dict(), checkpoint)\n",
        "                    else:\n",
        "                        self.logger.info(\"Epoch {:d}: val_loss did not improve from {:.4f}\".format(epoch + 1, -best_val_loss))\n",
        "            else:\n",
        "                torch.save(self.model.state_dict(), checkpoint)\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            self.logger.info(info_string.format(epoch + 1, self.epoch_num, epoch_time, -train_loss))\n",
        "\n",
        "            train_loss_history.append(train_loss)\n",
        "            val_loss_history.append(val_loss)\n",
        "\n",
        "        # train_linear_cca\n",
        "        if self.linear_cca is not None:\n",
        "            _, outputs = self._get_outputs(x1, x2)\n",
        "            self.linear_cca.fit(outputs[0], outputs[1], self.outdim_size)\n",
        "\n",
        "        checkpoint_ = torch.load(checkpoint)\n",
        "        self.model.load_state_dict(checkpoint_)\n",
        "\n",
        "        if vx1 is not None and vx2 is not None:\n",
        "            loss = self.transform(vx1, vx2)\n",
        "            self.logger.info(\"loss on validation data: {:.4f}\".format(-loss))\n",
        "\n",
        "        if tx1 is not None and tx2 is not None:\n",
        "            loss = self.transform(tx1, tx2)\n",
        "            self.logger.info('loss on test data: {:.4f}'.format(-loss))\n",
        "\n",
        "        fig = plt.figure(figsize=(10,4))\n",
        "        epochs = np.arange(1, self.epoch_num+1, step=1)\n",
        "        plt.plot(epochs, -1 * np.array(train_loss_history), color = 'tab:blue', label='Training')\n",
        "        plt.plot(epochs, -1 * np.array(val_loss_history), color = 'tab:orange', label='Validation')\n",
        "        plt.xticks(epochs)\n",
        "        plt.title('Model Correlation')\n",
        "        plt.ylabel('Correlation')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        return [-1 * np.array(train_loss_history), -1 * np.array(val_loss_history)]\n",
        "\n",
        "    def transform(self, x1, x2, use_linear_cca=False):\n",
        "        with torch.no_grad():\n",
        "            losses, outputs = self._get_outputs(x1, x2)\n",
        "\n",
        "            if use_linear_cca:\n",
        "                print(\"Linear CCA started!\")\n",
        "                outputs = self.linear_cca.transform(outputs[0], outputs[1])\n",
        "                return losses, outputs\n",
        "                # return np.mean(losses), outputs\n",
        "            else:\n",
        "                return np.mean(losses)\n",
        "\n",
        "    def _get_outputs(self, x1, x2):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            data_size = x1.size(0)\n",
        "            batch_idxs = list(BatchSampler(SequentialSampler(range(data_size)), batch_size=self.batch_size, drop_last=False))\n",
        "            losses = []\n",
        "            outputs1 = []\n",
        "            outputs2 = []\n",
        "            for batch_idx in batch_idxs:\n",
        "                batch_x1 = x1[batch_idx, :]\n",
        "                batch_x2 = x2[batch_idx, :]\n",
        "                out1, out2 = self.model(batch_x1, batch_x2, self.device)\n",
        "                outputs1.append(out1)\n",
        "                outputs2.append(out2)\n",
        "                loss = self.loss(out1, out2)\n",
        "                losses.append(loss.item())\n",
        "        outputs = [torch.cat(outputs1, dim=0).cpu().numpy(),\n",
        "                   torch.cat(outputs2, dim=0).cpu().numpy()]\n",
        "        return losses, outputs"
      ],
      "metadata": {
        "id": "X9krIZvDidgU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN"
      ],
      "metadata": {
        "id": "vdl4MMR-pB7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters Section"
      ],
      "metadata": {
        "id": "aonMr51FCHV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############\n",
        "\n",
        "device = torch.device('cuda')\n",
        "# print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
        "# print(\"GPU Name:\", torch.cuda.get_device_name())\n",
        "\n",
        "# the path to save the final learned features\n",
        "save_to = './new_features.gz'\n",
        "\n",
        "# the size of the new space learned by the model (number of the new features)\n",
        "outdim_size = 3\n",
        "\n",
        "# size of the input for view 1 and view 2\n",
        "input_shape1 = input_shape2 = 1\n",
        "\n",
        "# number of layers with nodes in each one\n",
        "# 5 components\n",
        "# layer_sizes1 = [256, 512, 1024, 1024, 1024, 512, outdim_size]\n",
        "# layer_sizes2 = [256, 512, 1024, 1024, 1024, 512, outdim_size]\n",
        "\n",
        "# 3 components\n",
        "layer_sizes1 = [128, 256, 512, 1024, outdim_size]\n",
        "layer_sizes2 = [128, 256, 512, 1024, outdim_size]\n",
        "\n",
        "# 2 components\n",
        "# layer_sizes1 = [128, 256, 512, outdim_size]\n",
        "# layer_sizes2 = [128, 256, 512, outdim_size]\n",
        "\n",
        "# the parameters for training the network\n",
        "learning_rate = 1e-3\n",
        "epoch_num = 10\n",
        "batch_size = 50\n",
        "\n",
        "# the regularization parameter of the network\n",
        "# necessary to avoid the gradient exploding/vanishing\n",
        "reg_par = 1e-5\n",
        "\n",
        "# if a linear CCA should get applied on the learned features\n",
        "apply_linear_cca = True\n",
        "\n",
        "############"
      ],
      "metadata": {
        "id": "klmdS4XTpDSv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ],
      "metadata": {
        "id": "R-Gj_6HlCLDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "# tick1 = 'AAPL'\n",
        "# tick2 = 'PFE'\n",
        "tick1 = '^GSPC'\n",
        "tick2 = '^DJI'\n",
        "start = '1992-01-01'\n",
        "end = '2023-01-01'\n",
        "\n",
        "data1 = fetch_data(tick1, start, end)\n",
        "data2 = fetch_data(tick2, start, end)\n",
        "\n",
        "print(data1.shape)\n",
        "print(data2.shape)\n",
        "\n",
        "data1_norm = (data1-data1.mean())/data1.std()\n",
        "data2_norm = (data2-data2.mean())/data2.std()\n",
        "\n",
        "train_ratio=0.7\n",
        "val_ratio=0.15\n",
        "data_length = len(data1)\n",
        "train_end = int(data_length * train_ratio)\n",
        "val_end = int(data_length * (train_ratio + val_ratio))\n",
        "\n",
        "idx1 = data1.index\n",
        "idx2 = data2.index\n",
        "\n",
        "# Plot complex data\n",
        "fig, ax = plt.subplots(3, 2)\n",
        "fig.set_size_inches(10,9)\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(hspace = 0.4, wspace = 0.2)\n",
        "\n",
        "ax[0][0].plot(data1)\n",
        "ax[0][0].axvline(x = idx1[train_end], alpha = 0.8, color='r', linewidth = 2, linestyle = '--')\n",
        "ax[0][0].axvline(x = idx1[val_end], alpha = 0.8, color='g', linewidth = 2, linestyle = '--')\n",
        "ax[0][0].set_xlabel('Date')\n",
        "ax[0][0].set_ylabel('Closing Price')\n",
        "ax[0][0].set_title('Data1')\n",
        "ax[0][0].grid(True)\n",
        "\n",
        "ax[0][1].plot(data2)\n",
        "ax[0][1].axvline(x = idx2[train_end], alpha = 0.8, color='r', linewidth = 2, linestyle = '--')\n",
        "ax[0][1].axvline(x = idx2[val_end], alpha = 0.8, color='g', linewidth = 2, linestyle = '--')\n",
        "ax[0][1].set_xlabel('Date')\n",
        "ax[0][1].set_ylabel('Closing Price')\n",
        "ax[0][1].set_title('Data2')\n",
        "ax[0][1].grid(True)\n",
        "\n",
        "ax[1][0].plot(data1.pct_change())\n",
        "ax[1][0].axvline(x = idx2[train_end], alpha = 0.8, color='r', linewidth = 2, linestyle = '--')\n",
        "ax[1][0].axvline(x = idx2[val_end], alpha = 0.8, color='g', linewidth = 2, linestyle = '--')\n",
        "ax[1][0].set_xlabel('Date')\n",
        "ax[1][0].set_ylabel('ROC')\n",
        "ax[1][0].set_title('Data1')\n",
        "ax[1][0].grid(True)\n",
        "\n",
        "ax[1][1].plot(data2.pct_change())\n",
        "ax[1][1].axvline(x = idx2[train_end], alpha = 0.8, color='r', linewidth = 2, linestyle = '--')\n",
        "ax[1][1].axvline(x = idx2[val_end], alpha = 0.8, color='g', linewidth = 2, linestyle = '--')\n",
        "ax[1][1].set_xlabel('Date')\n",
        "ax[1][1].set_ylabel('ROC')\n",
        "ax[1][1].set_title('Data2')\n",
        "ax[1][1].grid(True)\n",
        "\n",
        "view1 = load_data(tick1, start, end)\n",
        "view2 = load_data(tick2, start, end)\n",
        "\n",
        "circ_view1 = np.abs(np.mean(view1**2)/(np.mean(np.abs(view1**2)))); # Circularity coefficient of data\n",
        "ax[2][0].scatter(view1.real, view1.imag, marker='o', alpha=0.5)\n",
        "ax[2][0].set_xlabel('Real Part')\n",
        "ax[2][0].set_ylabel('Imaginary Part')\n",
        "ax[2][0].set_title('View 1 - Complex Data, \\u03B7 ={}'.format(round(circ_view1, 4)))\n",
        "ax[2][0].grid(True)\n",
        "\n",
        "circ_view2 = np.abs(np.mean(view2**2)/(np.mean(np.abs(view2**2)))); # Circularity coefficient of data\n",
        "ax[2][1].scatter(view2.real, view2.imag, marker='o', alpha=0.5)\n",
        "ax[2][1].set_xlabel('Real Part')\n",
        "ax[2][1].set_ylabel('Imaginary Part')\n",
        "ax[2][1].set_title('View 2 - Complex Data, \\u03B7 ={}'.format(round(circ_view2, 4)))\n",
        "ax[2][1].grid(True)\n",
        "\n",
        "# Split data\n",
        "train1, val1, test1 = split_data(view1, train_ratio=train_ratio, val_ratio=val_ratio)\n",
        "train2, val2, test2 = split_data(view2, train_ratio=train_ratio, val_ratio=val_ratio)\n",
        "\n",
        "fig.savefig(('indices.pdf'), bbox_inches='tight')"
      ],
      "metadata": {
        "id": "z2BAljEkoKo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "sdF3yQVWCQqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building, training, and producing the new features by CDCCA\n",
        "model = ComplexDeepCCA(layer_sizes1, layer_sizes2, input_shape1, input_shape2, outdim_size, device=device).double()\n",
        "l_cca = None\n",
        "if apply_linear_cca:\n",
        "    l_cca = linear_cca()\n",
        "trainer = Trainer(model, l_cca, outdim_size, epoch_num, batch_size, learning_rate, reg_par, device=device)\n",
        "\n",
        "history = trainer.train(train1, train2, val1, val2, test1, test2)"
      ],
      "metadata": {
        "id": "r7OhyWrDpK1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to visualize the\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "epochs = np.arange(1, epoch_num+1, step=1)\n",
        "epochs1 = np.arange(0, epoch_num+9, step=10)\n",
        "plt.plot(epochs, history[0], color = 'tab:blue', label='Training')\n",
        "plt.plot(epochs, history[1], color = 'tab:orange', label='Validation')\n",
        "plt.xticks(epochs1)\n",
        "plt.xlim(0, 52)\n",
        "plt.title('Model Correlation')\n",
        "plt.ylabel('Correlation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "QMrOI5FElbvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Canonical Variables and Correlation (loss)"
      ],
      "metadata": {
        "id": "gFPrq4lACW9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the views\n",
        "train_loss1, train_outputs = trainer.transform(train1, train2, apply_linear_cca)\n",
        "val_loss1, val_outputs = trainer.transform(val1, val2, apply_linear_cca)\n",
        "test_loss1, test_outputs = trainer.transform(test1, test2, apply_linear_cca)\n",
        "\n",
        "print(\"Training data canonical correlation\", -np.mean(train_loss1))\n",
        "print(\"Validation data canonical correlation\", -np.mean(val_loss1))\n",
        "print(\"Testing data canonical correlation\", -np.mean(test_loss1))"
      ],
      "metadata": {
        "id": "DZ6Ett0e9N2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output1 = [train_outputs[0], val_outputs[0], test_outputs[0]]\n",
        "output2 = [train_outputs[1], val_outputs[1], test_outputs[1]]\n",
        "\n",
        "dataset = [\"Training\", \"Validation\", \"Testing\"]\n",
        "\n",
        "# create figures for Real parts vs each each other\n",
        "fig = plt.figure(constrained_layout=True, figsize=(25, 10))\n",
        "subfigs = fig.subfigures(nrows=3, ncols=1)\n",
        "\n",
        "for row, subfig in enumerate(subfigs):\n",
        "    subfig.suptitle(f'{dataset[row]}', fontweight=\"bold\")\n",
        "\n",
        "    # create 1x3 subplots per subfig\n",
        "    axs = subfig.subplots(nrows=1, ncols=outdim_size)\n",
        "    for col, ax in enumerate(axs):\n",
        "        ax.scatter(output1[row][:, col].real, output2[row][:, col].real)\n",
        "\n",
        "        corr = np.corrcoef(output1[row][:, col].real, output2[row][:, col].real)[0, 1]\n",
        "        ax.set_title(\"Component {}\".format(col + 1))\n",
        "        # ax.set_title(\"Component {}, corr = {:.2f}\".format(col + 1, corr))\n",
        "        #ax.set_title(\"Component {}, corr = |{:.2f}|e^({:.2f}j)\".format(col + 1, mag, phase))\n",
        "        #ax.set_title(\"Component {}\".format(col + 1))\n",
        "        ax.set_xlabel('View 1')\n",
        "        ax.set_ylabel('View 2')\n",
        "        ax.grid()"
      ],
      "metadata": {
        "id": "tqRcc2vE62v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create figures for Imaginary parts vs each each other\n",
        "fig = plt.figure(constrained_layout=True, figsize=(15, 10))\n",
        "subfigs = fig.subfigures(nrows=3, ncols=1)\n",
        "\n",
        "for row, subfig in enumerate(subfigs):\n",
        "    subfig.suptitle(f'{dataset[row]}', fontweight=\"bold\")\n",
        "\n",
        "    # create 1x3 subplots per subfig\n",
        "    axs = subfig.subplots(nrows=1, ncols=outdim_size)\n",
        "    for col, ax in enumerate(axs):\n",
        "        ax.scatter(output1[row][:, col].imag, output2[row][:, col].imag)\n",
        "\n",
        "        ax.set_title(\"Component {}\".format(col + 1))\n",
        "        ax.set_xlabel('View 1')\n",
        "        ax.set_ylabel('View 2')\n",
        "        ax.grid()"
      ],
      "metadata": {
        "id": "_FdsIw8SyZjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting trials\n",
        "# create figures for Real vs Imaginary parts vs each each other\n",
        "fig = plt.figure(constrained_layout=True, figsize=(15, 10))\n",
        "subfigs = fig.subfigures(nrows=3, ncols=1)\n",
        "\n",
        "for row, subfig in enumerate(subfigs):\n",
        "    subfig.suptitle(f'{dataset[row]}', fontweight=\"bold\")\n",
        "\n",
        "    # create 1x3 subplots per subfig\n",
        "    axs = subfig.subplots(nrows=1, ncols=outdim_size)\n",
        "    for col, ax in enumerate(axs):\n",
        "        # ax.scatter(output1[row][:, col].real, output2[row][:, col].real)\n",
        "        ax.scatter(output1[row][:, col].real, output1[row][:, col].imag, marker='o', alpha=0.5, label = 'View 1')\n",
        "        ax.scatter(output2[row][:, col].real, output2[row][:, col].imag, marker='o', alpha=0.5, label = 'View 2')\n",
        "        ax.set_title(\"Component {}\".format(col + 1))\n",
        "        ax.set_xlabel('Real')\n",
        "        ax.set_ylabel('Imaginary')\n",
        "        ax.legend()\n",
        "        ax.grid()"
      ],
      "metadata": {
        "id": "E1YEvqrwCpRv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = torch.load('checkpoint.model')\n",
        "trainer.model.load_state_dict(d)\n",
        "trainer.model.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUAzwXCWDQ9q",
        "outputId": "5258153d-8f81-4339-8b56-1c865cbd96a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f1844145d90>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}